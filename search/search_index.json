{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to MkDocs","text":"<p>For full documentation visit mkdocs.org.</p>"},{"location":"#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"Models/Models/","title":"Models","text":""},{"location":"Models/Models/#models","title":"Models","text":"<p>This documentation show you how to models are handled with happy-vllm : </p> <p>This module contains the base Model class</p>"},{"location":"Models/Models/#src.happy_vllm.model.model_base.Model","title":"<code>Model</code>","text":"<p>Parent model class.</p> Source code in <code>src/happy_vllm/model/model_base.py</code> <pre><code>class Model:\n    \"\"\"Parent model class.\n    \"\"\"\n\n    def __init__(self, **kwargs):\n        '''Init. model class'''\n        self._model = None\n        self._tokenizer = None\n        self._model_conf = None\n        self._model_explainer = None\n        self._loaded = False\n\n    def is_model_loaded(self):\n        \"\"\"return the state of the model\"\"\"\n        return self._loaded\n\n    def loading(self, cli_args: Namespace, **kwargs):\n        \"\"\"load the model\"\"\"\n        self._load_model(cli_args, **kwargs)\n        self._loaded = True\n\n    def _load_model(self, cli_args: Namespace, **kwargs) -&gt; None:\n        \"\"\"Load a model from a file\n\n        Returns:\n            Tuple[Any, dict]: A tuple containing the model and a dict of metadata about it.\n        \"\"\"\n        settings = ModelSettings(**kwargs)\n        if settings.model != 'None':\n            cli_args.model = settings.model\n        if settings.model_name != '?':\n            model_name = settings.model_name\n        else:\n            model_name = cli_args.model_name\n\n        self._model_conf = {'model_name': model_name}\n\n        logger.info(f\"Loading the model from {cli_args.model}\")\n        if model_name != \"TEST MODEL\":\n            engine_args = AsyncEngineArgs.from_cli_args(cli_args) \n            self._model = AsyncLLMEngine.from_engine_args(engine_args) # type: ignore\n            if isinstance(self._model.engine.tokenizer, TokenizerGroup): # type: ignore\n                self._tokenizer = self._model.engine.tokenizer.tokenizer # type: ignore\n            else:\n                self._tokenizer = self._model.engine.tokenizer # type: ignore\n            self._tokenizer_lmformatenforcer = build_token_enforcer_tokenizer_data(self._tokenizer)\n            self.max_model_len = self._model.engine.model_config.max_model_len # type: ignore\n            self.original_truncation_side = self._tokenizer.truncation_side\n        # For test purpose\n        else:\n            self.max_model_len = 2048\n            self.original_truncation_side = 'right'\n            self._tokenizer = AutoTokenizer.from_pretrained(settings.tokenizer_name,\n                                                     cache_dir=settings.TEST_MODELS_DIR, truncation_side=self.original_truncation_side)\n            self._tokenizer_lmformatenforcer = build_token_enforcer_tokenizer_data(self._tokenizer)\n            self._model = MockModel(self._tokenizer)\n        logger.info(f\"Model loaded\")\n\n    def tokenize(self, text: str) -&gt; List[int]:\n        \"\"\"Tokenizes a text\n\n        Args:\n            text (str) : The text to tokenize\n\n        Returns:\n            list : The list of token ids\n        \"\"\"\n        return list(utils.proper_tokenization(self._tokenizer, text))\n\n    def split_text(self, text: str, num_tokens_in_chunk: int = 200, separators: Union[list, None] = None) -&gt; List[str]:\n        '''Splits a text in small texts containing at least num_tokens_in_chunk tokens and ending by a separator. note that the `separators` \n        used are the tokenization of the strings and not the strings themselves (which explains why we must for example \n        specify ' .' and '.' as two separate separators) \n\n        Args:\n            text (str) : The text to split\n\n        Kwargs:\n            num_tokens_in_chunk (int) : The minimal number of tokens in the chunk\n            separators (list) : The separators marking the end of a sentence\n\n        Returns:\n            A list of strings each string containing at least num_tokens_in_chunk tokens and ending by a separator\n        '''\n        if separators is None:\n            separators = [\".\", \"!\", \"?\", \"|\", \" .\", \" !\", \" ?\", \" |\"]\n        separators_tokens_ids = set()\n        for separator in separators:\n            separators_tokens_ids.add(utils.proper_tokenization(self._tokenizer, separator))\n        tokens = list(utils.proper_tokenization(self._tokenizer, text))\n        indices_separators = []\n        for separator_tokens_ids in separators_tokens_ids:\n            indices_separators += find_indices_sub_list_in_list(tokens, list(separator_tokens_ids))\n        indices_separators.sort()\n\n        chunks = []\n        index_beginning_chunk = 0\n        current_used_separator = 0\n        while current_used_separator &lt; len(indices_separators):\n            index_current_used_separator = indices_separators[current_used_separator]\n            if index_current_used_separator +1 - index_beginning_chunk &gt;= num_tokens_in_chunk:\n                chunks.append(tokens[index_beginning_chunk:index_current_used_separator + 1])\n                index_beginning_chunk = index_current_used_separator + 1\n            current_used_separator += 1\n        chunks.append(tokens[index_beginning_chunk:])\n        chunks = [utils.proper_decode(self._tokenizer, chunk) for chunk in chunks]\n        chunks = [element for element in chunks if element!= \"\"]\n        return chunks\n\n    def extract_text_outside_truncation(self, text: str, truncation_side: Union[str, None] = None, max_length: Union[int, None] = None) -&gt; str:\n        \"\"\"Extracts the part of the prompt not kept after truncation, which will not be infered by the model.\n        First, we tokenize the prompt while applying truncation.\n        We obtain a list of sequences of token ids padded, which are outside the truncation.\n        Then we decode this list of tensors of token IDs containing special tokens to a string.\n\n        Args:\n            prompt (str)\n\n        Returns:\n            prompt outside the truncation (str)\n        \"\"\"\n        if max_length is None:\n            max_length = self.max_model_len\n        if truncation_side is None:\n            truncation_side = self.original_truncation_side\n        self._tokenizer.truncation_side = truncation_side\n        list_tokens = self._tokenizer(text, truncation=True, add_special_tokens=False, max_length=max_length, return_overflowing_tokens=True)['input_ids']\n        if len(list_tokens) &lt;= 1:\n            return ''\n        not_truncated = list_tokens[0]\n        truncated_tmp = list_tokens[1:]\n        if self._tokenizer.truncation_side == 'left':\n            truncated_tmp.reverse()\n        truncated = []\n        for truncated_tokens in truncated_tmp:\n            truncated += truncated_tokens\n        truncated_str = self._tokenizer.decode(truncated)\n        self._tokenizer.truncation_side = self.original_truncation_side\n        return truncated_str\n\n    def get_gpu_kv_cache_usage(self) -&gt; float:\n        \"\"\"Gets the GPU KV cache usage\n\n        Returns:\n            The GPU KV cache usage\n        \"\"\"\n        total_num_gpu_blocks = self._model.engine.cache_config.num_gpu_blocks\n        num_free_gpu_blocks = (\n            self._model.engine.scheduler.block_manager.get_num_free_gpu_blocks())\n        num_used_gpu_blocks = total_num_gpu_blocks - num_free_gpu_blocks\n        gpu_cache_usage = num_used_gpu_blocks / total_num_gpu_blocks\n        return gpu_cache_usage\n\n    def get_cpu_kv_cache_usage(self) -&gt; float:\n        \"\"\"Gets the CPU KV cache usage\n\n        Returns:\n            The CPU KV cache usage\n        \"\"\"\n        total_num_cpu_blocks = self._model.engine.cache_config.num_cpu_blocks\n        if total_num_cpu_blocks &gt; 0:\n            num_free_cpu_blocks = (\n                self._model.engine.scheduler.block_manager.get_num_free_cpu_blocks())\n            num_used_cpu_blocks = total_num_cpu_blocks - num_free_cpu_blocks\n            cpu_cache_usage = num_used_cpu_blocks / total_num_cpu_blocks\n        else:\n            cpu_cache_usage = 0.0\n        return cpu_cache_usage\n\n    def get_status_requests(self) -&gt; dict:\n        \"\"\"Gets the status of the different requests being processed\n\n        Returns:\n            A dictionary containing the number of requests in the different status (running, swapped and pending)\n        \"\"\"\n        status_requests = {\"requests_running\": len(self._model.engine.scheduler.running),\n                            \"requests_swapped\": len(self._model.engine.scheduler.swapped),\n                            \"requests_pending\": len(self._model.engine.scheduler.waiting)}\n        return status_requests\n</code></pre>"},{"location":"Models/Models/#src.happy_vllm.model.model_base.Model.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Init. model class</p> Source code in <code>src/happy_vllm/model/model_base.py</code> <pre><code>def __init__(self, **kwargs):\n    '''Init. model class'''\n    self._model = None\n    self._tokenizer = None\n    self._model_conf = None\n    self._model_explainer = None\n    self._loaded = False\n</code></pre>"},{"location":"Models/Models/#src.happy_vllm.model.model_base.Model.extract_text_outside_truncation","title":"<code>extract_text_outside_truncation(text, truncation_side=None, max_length=None)</code>","text":"<p>Extracts the part of the prompt not kept after truncation, which will not be infered by the model. First, we tokenize the prompt while applying truncation. We obtain a list of sequences of token ids padded, which are outside the truncation. Then we decode this list of tensors of token IDs containing special tokens to a string.</p> <p>Returns:</p> Type Description <code>str</code> <p>prompt outside the truncation (str)</p> Source code in <code>src/happy_vllm/model/model_base.py</code> <pre><code>def extract_text_outside_truncation(self, text: str, truncation_side: Union[str, None] = None, max_length: Union[int, None] = None) -&gt; str:\n    \"\"\"Extracts the part of the prompt not kept after truncation, which will not be infered by the model.\n    First, we tokenize the prompt while applying truncation.\n    We obtain a list of sequences of token ids padded, which are outside the truncation.\n    Then we decode this list of tensors of token IDs containing special tokens to a string.\n\n    Args:\n        prompt (str)\n\n    Returns:\n        prompt outside the truncation (str)\n    \"\"\"\n    if max_length is None:\n        max_length = self.max_model_len\n    if truncation_side is None:\n        truncation_side = self.original_truncation_side\n    self._tokenizer.truncation_side = truncation_side\n    list_tokens = self._tokenizer(text, truncation=True, add_special_tokens=False, max_length=max_length, return_overflowing_tokens=True)['input_ids']\n    if len(list_tokens) &lt;= 1:\n        return ''\n    not_truncated = list_tokens[0]\n    truncated_tmp = list_tokens[1:]\n    if self._tokenizer.truncation_side == 'left':\n        truncated_tmp.reverse()\n    truncated = []\n    for truncated_tokens in truncated_tmp:\n        truncated += truncated_tokens\n    truncated_str = self._tokenizer.decode(truncated)\n    self._tokenizer.truncation_side = self.original_truncation_side\n    return truncated_str\n</code></pre>"},{"location":"Models/Models/#src.happy_vllm.model.model_base.Model.get_cpu_kv_cache_usage","title":"<code>get_cpu_kv_cache_usage()</code>","text":"<p>Gets the CPU KV cache usage</p> <p>Returns:</p> Type Description <code>float</code> <p>The CPU KV cache usage</p> Source code in <code>src/happy_vllm/model/model_base.py</code> <pre><code>def get_cpu_kv_cache_usage(self) -&gt; float:\n    \"\"\"Gets the CPU KV cache usage\n\n    Returns:\n        The CPU KV cache usage\n    \"\"\"\n    total_num_cpu_blocks = self._model.engine.cache_config.num_cpu_blocks\n    if total_num_cpu_blocks &gt; 0:\n        num_free_cpu_blocks = (\n            self._model.engine.scheduler.block_manager.get_num_free_cpu_blocks())\n        num_used_cpu_blocks = total_num_cpu_blocks - num_free_cpu_blocks\n        cpu_cache_usage = num_used_cpu_blocks / total_num_cpu_blocks\n    else:\n        cpu_cache_usage = 0.0\n    return cpu_cache_usage\n</code></pre>"},{"location":"Models/Models/#src.happy_vllm.model.model_base.Model.get_gpu_kv_cache_usage","title":"<code>get_gpu_kv_cache_usage()</code>","text":"<p>Gets the GPU KV cache usage</p> <p>Returns:</p> Type Description <code>float</code> <p>The GPU KV cache usage</p> Source code in <code>src/happy_vllm/model/model_base.py</code> <pre><code>def get_gpu_kv_cache_usage(self) -&gt; float:\n    \"\"\"Gets the GPU KV cache usage\n\n    Returns:\n        The GPU KV cache usage\n    \"\"\"\n    total_num_gpu_blocks = self._model.engine.cache_config.num_gpu_blocks\n    num_free_gpu_blocks = (\n        self._model.engine.scheduler.block_manager.get_num_free_gpu_blocks())\n    num_used_gpu_blocks = total_num_gpu_blocks - num_free_gpu_blocks\n    gpu_cache_usage = num_used_gpu_blocks / total_num_gpu_blocks\n    return gpu_cache_usage\n</code></pre>"},{"location":"Models/Models/#src.happy_vllm.model.model_base.Model.get_status_requests","title":"<code>get_status_requests()</code>","text":"<p>Gets the status of the different requests being processed</p> <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary containing the number of requests in the different status (running, swapped and pending)</p> Source code in <code>src/happy_vllm/model/model_base.py</code> <pre><code>def get_status_requests(self) -&gt; dict:\n    \"\"\"Gets the status of the different requests being processed\n\n    Returns:\n        A dictionary containing the number of requests in the different status (running, swapped and pending)\n    \"\"\"\n    status_requests = {\"requests_running\": len(self._model.engine.scheduler.running),\n                        \"requests_swapped\": len(self._model.engine.scheduler.swapped),\n                        \"requests_pending\": len(self._model.engine.scheduler.waiting)}\n    return status_requests\n</code></pre>"},{"location":"Models/Models/#src.happy_vllm.model.model_base.Model.is_model_loaded","title":"<code>is_model_loaded()</code>","text":"<p>return the state of the model</p> Source code in <code>src/happy_vllm/model/model_base.py</code> <pre><code>def is_model_loaded(self):\n    \"\"\"return the state of the model\"\"\"\n    return self._loaded\n</code></pre>"},{"location":"Models/Models/#src.happy_vllm.model.model_base.Model.loading","title":"<code>loading(cli_args, **kwargs)</code>","text":"<p>load the model</p> Source code in <code>src/happy_vllm/model/model_base.py</code> <pre><code>def loading(self, cli_args: Namespace, **kwargs):\n    \"\"\"load the model\"\"\"\n    self._load_model(cli_args, **kwargs)\n    self._loaded = True\n</code></pre>"},{"location":"Models/Models/#src.happy_vllm.model.model_base.Model.split_text","title":"<code>split_text(text, num_tokens_in_chunk=200, separators=None)</code>","text":"<p>Splits a text in small texts containing at least num_tokens_in_chunk tokens and ending by a separator. note that the <code>separators</code>  used are the tokenization of the strings and not the strings themselves (which explains why we must for example  specify ' .' and '.' as two separate separators) </p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str) </code> <p>The text to split</p> required Kwargs <p>num_tokens_in_chunk (int) : The minimal number of tokens in the chunk separators (list) : The separators marking the end of a sentence</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>A list of strings each string containing at least num_tokens_in_chunk tokens and ending by a separator</p> Source code in <code>src/happy_vllm/model/model_base.py</code> <pre><code>def split_text(self, text: str, num_tokens_in_chunk: int = 200, separators: Union[list, None] = None) -&gt; List[str]:\n    '''Splits a text in small texts containing at least num_tokens_in_chunk tokens and ending by a separator. note that the `separators` \n    used are the tokenization of the strings and not the strings themselves (which explains why we must for example \n    specify ' .' and '.' as two separate separators) \n\n    Args:\n        text (str) : The text to split\n\n    Kwargs:\n        num_tokens_in_chunk (int) : The minimal number of tokens in the chunk\n        separators (list) : The separators marking the end of a sentence\n\n    Returns:\n        A list of strings each string containing at least num_tokens_in_chunk tokens and ending by a separator\n    '''\n    if separators is None:\n        separators = [\".\", \"!\", \"?\", \"|\", \" .\", \" !\", \" ?\", \" |\"]\n    separators_tokens_ids = set()\n    for separator in separators:\n        separators_tokens_ids.add(utils.proper_tokenization(self._tokenizer, separator))\n    tokens = list(utils.proper_tokenization(self._tokenizer, text))\n    indices_separators = []\n    for separator_tokens_ids in separators_tokens_ids:\n        indices_separators += find_indices_sub_list_in_list(tokens, list(separator_tokens_ids))\n    indices_separators.sort()\n\n    chunks = []\n    index_beginning_chunk = 0\n    current_used_separator = 0\n    while current_used_separator &lt; len(indices_separators):\n        index_current_used_separator = indices_separators[current_used_separator]\n        if index_current_used_separator +1 - index_beginning_chunk &gt;= num_tokens_in_chunk:\n            chunks.append(tokens[index_beginning_chunk:index_current_used_separator + 1])\n            index_beginning_chunk = index_current_used_separator + 1\n        current_used_separator += 1\n    chunks.append(tokens[index_beginning_chunk:])\n    chunks = [utils.proper_decode(self._tokenizer, chunk) for chunk in chunks]\n    chunks = [element for element in chunks if element!= \"\"]\n    return chunks\n</code></pre>"},{"location":"Models/Models/#src.happy_vllm.model.model_base.Model.tokenize","title":"<code>tokenize(text)</code>","text":"<p>Tokenizes a text</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str) </code> <p>The text to tokenize</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>List[int]</code> <p>The list of token ids</p> Source code in <code>src/happy_vllm/model/model_base.py</code> <pre><code>def tokenize(self, text: str) -&gt; List[int]:\n    \"\"\"Tokenizes a text\n\n    Args:\n        text (str) : The text to tokenize\n\n    Returns:\n        list : The list of token ids\n    \"\"\"\n    return list(utils.proper_tokenization(self._tokenizer, text))\n</code></pre>"},{"location":"Models/Models/#src.happy_vllm.model.model_base.ModelSettings","title":"<code>ModelSettings</code>","text":"<p>             Bases: <code>BaseSettings</code></p> <p>Download settings</p> <p>This class is used for settings management purpose, have a look at the pydantic documentation for more details : https://pydantic-docs.helpmanual.io/usage/settings/</p> <p>By default, it looks for environment variables (case insensitive) to set the settings if a variable is not found, it looks for a file name .env in your working directory where you can declare the values of the variables and finally it sets the values to the default ones one can define above</p> Source code in <code>src/happy_vllm/model/model_base.py</code> <pre><code>class ModelSettings(BaseSettings):\n    \"\"\"Download settings\n\n    This class is used for settings management purpose, have a look at the pydantic\n    documentation for more details : https://pydantic-docs.helpmanual.io/usage/settings/\n\n    By default, it looks for environment variables (case insensitive) to set the settings\n    if a variable is not found, it looks for a file name .env in your working directory\n    where you can declare the values of the variables and finally it sets the values\n    to the default ones one can define above\n    \"\"\"\n    model : str = 'None'\n    model_name : str = '?'\n    tokenizer_name : str = 'None'\n    TEST_MODELS_DIR : str = 'None'\n\n    model_config = SettingsConfigDict(env_file=\".env\", extra='ignore', protected_namespaces=('settings', ))\n</code></pre>"},{"location":"Models/Models/#src.happy_vllm.model.model_base.find_indices_sub_list_in_list","title":"<code>find_indices_sub_list_in_list(big_list, sub_list)</code>","text":"<p>Find the indices of the presence of a sub list in a bigger list. For example if big_list = [3, 4, 1, 2, 3, 4, 5, 6, 3, 4] and sub_list = [3, 4], the result will be [1, 5, 9]</p> <p>Parameters:</p> Name Type Description Default <code>big_list</code> <code>list) </code> <p>The list in which we want to find the sub_list</p> required <code>sub_list</code> <code>list</code> <p>The list we want the indices of in the big_list</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>The list of indices of where the sub_list is in the big_list</p> Source code in <code>src/happy_vllm/model/model_base.py</code> <pre><code>def find_indices_sub_list_in_list(big_list: list, sub_list: list) -&gt; list:\n    \"\"\"Find the indices of the presence of a sub list in a bigger list. For example\n    if big_list = [3, 4, 1, 2, 3, 4, 5, 6, 3, 4] and sub_list = [3, 4],\n    the result will be [1, 5, 9]\n\n    Args:\n        big_list (list) : The list in which we want to find the sub_list\n        sub_list (list): The list we want the indices of in the big_list\n\n    Returns:\n        list : The list of indices of where the sub_list is in the big_list \n    \"\"\"\n    len_sub_list = len(sub_list)\n    indices = []\n    for index in range(len(big_list)):\n        if big_list[index - len_sub_list + 1: index + 1] == sub_list:\n            indices.append(index)\n    return indices\n</code></pre>"},{"location":"Quick_start/Quick_start/","title":"Getting started","text":""},{"location":"Quick_start/Quick_start/#quick-start","title":"Quick start","text":"<p>This documentation show you how to quicky get started with happy-vllm :</p> <ul> <li>Paragraph A</li> <li>Paragraph B</li> <li>Paragraph C</li> </ul>"},{"location":"Quick_start/Quick_start/#src.happy_vllm.utils.get_package_version","title":"<code>get_package_version()</code>","text":"<p>Returns the current version of the package</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>version of the package</p> Source code in <code>src/happy_vllm/utils.py</code> <pre><code>def get_package_version() -&gt; str:\n    '''Returns the current version of the package\n\n    Returns:\n        str: version of the package\n    '''\n    version = importlib.metadata.version(\"happy_vllm\")\n    return version\n</code></pre>"},{"location":"Quick_start/Quick_start/#src.happy_vllm.utils.proper_decode","title":"<code>proper_decode(tokenizer, token_ids)</code>","text":"<p>Gets the corresponding string from the token ids. We must use this technique in order to get the right string with their whitespace in the case of some tokenizers (e.g. Llama) deleting whitespace in front of the tokenized sentence</p> Source code in <code>src/happy_vllm/utils.py</code> <pre><code>def proper_decode(tokenizer, token_ids: Union[int, List[int]]) -&gt; str:\n    \"\"\"Gets the corresponding string from the token ids. We must use this technique in order to get the right string\n    with their whitespace in the case of some tokenizers (e.g. Llama) deleting whitespace in front of the tokenized sentence\n    \"\"\"\n    if isinstance(token_ids, int):\n        token_ids = [token_ids]\n    extra_token_id = proper_tokenization(tokenizer, 'c')[0]\n    token_ids = [extra_token_id] + token_ids\n    return tokenizer.decode(token_ids)[1:]\n</code></pre>"},{"location":"Quick_start/Quick_start/#src.happy_vllm.utils.proper_tokenization","title":"<code>proper_tokenization(tokenizer, str_to_tokenize)</code>","text":"<p>Gets the token ids for a str. We must use this technique in order to get the right token ids with their whitespace in the case of some tokenizers (e.g. Llama) adding whitespace in front of the tokenized sentence</p> <p>Parameters:</p> Name Type Description Default <code>tokenizer</code> <p>A tokenizer</p> required <code>str_to_tokenizer</code> <code>str) </code> <p>The string one wants to tokenize</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <code>tuple</code> <p>The tuple containing the token ids for the input string</p> Source code in <code>src/happy_vllm/utils.py</code> <pre><code>def proper_tokenization(tokenizer, str_to_tokenize: str) -&gt; tuple:\n    \"\"\"Gets the token ids for a str. We must use this technique in order to get the right token ids\n    with their whitespace in the case of some tokenizers (e.g. Llama) adding whitespace in front of the tokenized sentence\n\n    Args:\n        tokenizer : A tokenizer\n        str_to_tokenizer (str) : The string one wants to tokenize\n\n    Returns:\n        tuple : The tuple containing the token ids for the input string\n    \"\"\"\n    big_word = 'thisisareallybigwordisntit'\n    token_ids_big_word = tokenizer(big_word, add_special_tokens=False)['input_ids']\n    # We concatenate the big word with the str in order for the str not to be at the beginning of the sentence\n    new_text = big_word + str_to_tokenize\n    token_ids_new_text = tokenizer(new_text, add_special_tokens=False)['input_ids']\n    # We check that part of the str was not \"integrated\" in a token of the big_word\n    while token_ids_big_word != token_ids_new_text[:len(token_ids_big_word)]:\n        # If it has been \"integrated\", we take another big_word\n        big_word = big_word[:-1]\n        token_ids_big_word = tokenizer(big_word, add_special_tokens=False)['input_ids']\n        # We concatenate the big word with the str in order for the str not to be at the beginning of the sentence\n        new_text = big_word + str_to_tokenize\n        token_ids_new_text = tokenizer(new_text, add_special_tokens=False)['input_ids']\n    token_ids_str = tuple(token_ids_new_text[len(token_ids_big_word):])\n    return token_ids_str\n</code></pre>"},{"location":"Serving/Serving/","title":"Serving","text":""},{"location":"Serving/Serving/#serving","title":"Serving","text":"<p>This documentation show you how serving is handled with happy-vllm : </p>"},{"location":"Serving/Serving/#src.happy_vllm.routers.functional.decode","title":"<code>decode(request)</code>  <code>async</code>","text":"<p>Decodes token ids</p> <p>The request should be a JSON object with the following fields: - token_ids: The ids of the tokens - with_tokens_str : If the result should also include a list of str - vanilla (optional) : Whether we want the vanilla version of the tokenizers</p> Source code in <code>src/happy_vllm/routers/functional.py</code> <pre><code>@router.post(\"/decode\")\nasync def decode(request: Request) -&gt; Response:\n    \"\"\"Decodes token ids\n\n    The request should be a JSON object with the following fields:\n    - token_ids: The ids of the tokens\n    - with_tokens_str : If the result should also include a list of str\n    - vanilla (optional) : Whether we want the vanilla version of the tokenizers\n    \"\"\"\n    model: Model = RESOURCES.get(RESOURCE_MODEL)\n    request_dict = await request.json()\n    token_ids = request_dict.pop(\"token_ids\")\n    with_tokens_str = request_dict.get(\"with_tokens_str\", False)\n    vanilla = request_dict.get(\"vanilla\", True)\n\n    if vanilla:\n        decoded_string = model._tokenizer.decode(token_ids)\n        if with_tokens_str:\n            tokens_str = model._tokenizer.convert_ids_to_tokens(token_ids)\n    else:\n        decoded_string = utils.proper_decode(model._tokenizer, token_ids)\n        if with_tokens_str:\n            tokens_str = [utils.proper_decode(model._tokenizer, token_id) for token_id in token_ids]\n\n\n    ret = {\"decoded_string\": decoded_string}\n    if with_tokens_str:\n        ret[ \"tokens_str\"] = tokens_str\n    return JSONResponse(ret)\n</code></pre>"},{"location":"Serving/Serving/#src.happy_vllm.routers.functional.generate","title":"<code>generate(request)</code>  <code>async</code>","text":"<p>Generate completion for the request.</p> <p>The request should be a JSON object with the following fields: - prompt: The prompt to use for the generation. - other fields: The sampling parameters (See <code>SamplingParams</code> for details).</p> Source code in <code>src/happy_vllm/routers/functional.py</code> <pre><code>@router.post(\"/generate\")\nasync def generate(request: Request) -&gt; Response:\n    \"\"\"Generate completion for the request.\n\n    The request should be a JSON object with the following fields:\n    - prompt: The prompt to use for the generation.\n    - other fields: The sampling parameters (See `SamplingParams` for details).\n    \"\"\"\n\n    model: Model = RESOURCES.get(RESOURCE_MODEL)\n    request_dict = await request.json()\n    prompt, prompt_in_response, sampling_params = parse_generate_parameters(request_dict, model._model, model._tokenizer, model._tokenizer_lmformatenforcer)\n    request_id = random_uuid()\n    model._tokenizer.truncation_side = model.original_truncation_side\n    results_generator = model._model.generate(prompt, sampling_params, request_id)\n\n    # Non-streaming case\n    final_output = None\n    async for request_output in results_generator:\n        if await request.is_disconnected():\n            # Abort the request if the client disconnects.\n            await model._model.abort(request_id)\n            return Response(status_code=499)\n        final_output = request_output\n\n    if final_output is None:\n        raise ValueError('The final ouput is None')\n    prompt = final_output.prompt\n    text_outputs = [output.text for output in final_output.outputs]\n    finish_reasons = [output.finish_reason for output in request_output.outputs]\n    finish_reasons = [\"None\" if finish_reason is None else finish_reason for finish_reason in finish_reasons]\n    ret = {\"responses\": text_outputs, \"finish_reasons\": finish_reasons}\n    if prompt_in_response:\n        ret['prompt'] = prompt\n    return JSONResponse(ret)\n</code></pre>"},{"location":"Serving/Serving/#src.happy_vllm.routers.functional.generate_stream","title":"<code>generate_stream(request)</code>  <code>async</code>","text":"<p>Generate completion for the request.</p> <p>The request should be a JSON object with the following fields: - prompt: The prompt to use for the generation. - other fields: The sampling parameters (See <code>SamplingParams</code> for details).</p> Source code in <code>src/happy_vllm/routers/functional.py</code> <pre><code>@router.post(\"/generate_stream\")\nasync def generate_stream(request: Request) -&gt; StreamingResponse:\n    \"\"\"Generate completion for the request.\n\n    The request should be a JSON object with the following fields:\n    - prompt: The prompt to use for the generation.\n    - other fields: The sampling parameters (See `SamplingParams` for details).\n    \"\"\"\n    model: Model = RESOURCES.get(RESOURCE_MODEL)\n    request_dict = await request.json()\n    prompt, prompt_in_response, sampling_params = parse_generate_parameters(request_dict, model._model, model._tokenizer, model._tokenizer_lmformatenforcer)\n    request_id = random_uuid()\n    model._tokenizer.truncation_side = model.original_truncation_side\n    results_generator = model._model.generate(prompt, sampling_params, request_id)\n\n    async def stream_results() -&gt; AsyncGenerator[str, None]:\n        async for request_output in results_generator:\n            prompt = request_output.prompt\n            text_outputs = [\n                output.text for output in request_output.outputs\n            ]\n            finish_reasons = [output.finish_reason for output in request_output.outputs]\n            finish_reasons = [\"None\" if finish_reason is None else finish_reason for finish_reason in finish_reasons]\n            ret = {\"responses\": text_outputs, \"finish_reasons\": finish_reasons}\n            if prompt_in_response:\n                ret['prompt'] = prompt\n            yield (json.dumps(ret) + \"\\n\")#.encode(\"utf-8\")\n\n    return StreamingResponse(stream_results())\n</code></pre>"},{"location":"Serving/Serving/#src.happy_vllm.routers.functional.metadata_text","title":"<code>metadata_text(request)</code>  <code>async</code>","text":"<p>Gives meta data on a text</p> <p>The request should be a JSON object with the following fields: - text: The text to parse - truncation_side (optional): The truncation side of the tokenizer - max_length (optional) : The max length before truncation</p> <p>The default values for truncation_side and max_length are those of the underlying model</p> Source code in <code>src/happy_vllm/routers/functional.py</code> <pre><code>@router.post(\"/metadata_text\")\nasync def metadata_text(request: Request):\n    \"\"\"Gives meta data on a text\n\n    The request should be a JSON object with the following fields:\n    - text: The text to parse\n    - truncation_side (optional): The truncation side of the tokenizer\n    - max_length (optional) : The max length before truncation\n\n    The default values for truncation_side and max_length are those of the underlying model\n    \"\"\"\n    model: Model = RESOURCES.get(RESOURCE_MODEL)\n\n    request_dict = await request.json()\n\n    tokens_ids = model.tokenize(request_dict['text'])\n    truncated_text = model.extract_text_outside_truncation(**request_dict)\n    ret = {\"tokens_nb\": len(tokens_ids), \"truncated_text\": truncated_text}\n\n    return JSONResponse(ret)\n</code></pre>"},{"location":"Serving/Serving/#src.happy_vllm.routers.functional.parse_generate_parameters","title":"<code>parse_generate_parameters(request_dict, model, tokenizer, tokenizer_lmformatenforcer)</code>","text":"<p>Parses the body of the request to obtain the prompt and the sampling parameters</p> <p>Parameters:</p> Name Type Description Default <code>request_dict</code> <code>dict</code> <p>The body of the request</p> required <code>model</code> <code>AsyncLLMEngine</code> <p>The model</p> required <code>tokenizer</code> <p>The tokenizer</p> required <code>tokenizer_lmformatenforcer</code> <p>The LM format enforcer version of the tokenizer</p> required <p>Returns:     str : The prompt     bool : Whether the prompt should be displayed in the response     SamplingParams : The vllm sampling parameters</p> Source code in <code>src/happy_vllm/routers/functional.py</code> <pre><code>def parse_generate_parameters(request_dict: dict, model: AsyncLLMEngine, tokenizer: PreTrainedTokenizerBase,\n                            tokenizer_lmformatenforcer: TokenEnforcerTokenizerData) -&gt; Tuple[str, bool, SamplingParams]:\n    \"\"\"Parses the body of the request to obtain the prompt and the sampling parameters\n\n    Args:\n        request_dict (dict): The body of the request\n        model (AsyncLLMEngine): The model\n        tokenizer : The tokenizer\n        tokenizer_lmformatenforcer : The LM format enforcer version of the tokenizer\n    Returns:\n        str : The prompt\n        bool : Whether the prompt should be displayed in the response\n        SamplingParams : The vllm sampling parameters\n    \"\"\"\n    prompt = request_dict.pop(\"prompt\")\n    if 'prompt_in_response' in request_dict:\n        prompt_in_response = request_dict.pop('prompt_in_response')\n    else:\n        prompt_in_response = False\n    detect_logits_processors_incompatibilities(request_dict)\n    logits_processors = parse_logits_processors(request_dict, prompt, model, tokenizer, tokenizer_lmformatenforcer)\n    sampling_params = SamplingParams(**request_dict)\n    sampling_params.logits_processors = logits_processors\n    for logits_processor in logits_processors:\n        if isinstance(logits_processor, VLLMLogitsProcessorMinTokens):\n            min_tokens = logits_processor.min_tokens\n            if min_tokens &gt; sampling_params.max_tokens:\n                raise ValueError(f\"min_tokens : {min_tokens} can't be superior to max_tokens : {sampling_params.max_tokens}\")\n    return prompt, prompt_in_response, sampling_params\n</code></pre>"},{"location":"Serving/Serving/#src.happy_vllm.routers.functional.parse_logits_processors","title":"<code>parse_logits_processors(request_dict, prompt, model, tokenizer, tokenizer_lmformatenforcer)</code>","text":"<p>Parses the body of the request in order to provide the logits processors</p> <p>Parameters:</p> Name Type Description Default <code>request_dict</code> <code>dict</code> <p>The body of the request</p> required <code>model</code> <code>AsyncLLMEngine</code> <p>The model</p> required <code>tokenizer</code> <p>The tokenizer</p> required <code>tokenizer_lmformatenforcer</code> <p>The LM format enforcer version of the tokenizer</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>The list of logits processors</p> Source code in <code>src/happy_vllm/routers/functional.py</code> <pre><code>def parse_logits_processors(request_dict: dict, prompt: str, model: AsyncLLMEngine, tokenizer: PreTrainedTokenizerBase,\n                            tokenizer_lmformatenforcer: TokenEnforcerTokenizerData) -&gt; list:\n    \"\"\"Parses the body of the request in order to provide the logits processors\n\n    Args:\n        request_dict (dict): The body of the request\n        model (AsyncLLMEngine): The model\n        tokenizer : The tokenizer\n        tokenizer_lmformatenforcer : The LM format enforcer version of the tokenizer\n\n    Returns:\n        list : The list of logits processors\n    \"\"\"\n    logits_processors = []\n    references = {'prompt': prompt, 'model': model, 'tokenizer': tokenizer, 'tokenizer_lmformatenforcer': tokenizer_lmformatenforcer}\n    list_keyword = list(request_dict)\n    for keyword_main in list_keyword:\n        # Parse the body sent via API as explained in happy_vllm.logits_processors.utils_parse_logits_processors\n        if keyword_main in logits_processors_parser:\n            dict_logits_processor = logits_processors_parser[keyword_main]\n            class_to_instantiate = dict_logits_processor['class']\n            kwargs = {}     \n            # Add prompt, model or tokenizer if needed\n            for reference, refered_object in references.items():\n                if dict_logits_processor.get(reference, False):\n                    kwargs[reference] = refered_object\n            # Add arguments to instantiate the logits_processor if needed\n            for keyword, class_argument in dict_logits_processor['arguments'].items():\n                if keyword in request_dict:\n                    kwargs[class_argument] = request_dict.pop(keyword)\n            logit_processor = class_to_instantiate(**kwargs)\n            logits_processors.append(logit_processor)\n    # To ensure that no unsuitable keywords are passed to vllm, we pop them\n    for keyword_main, logits_processor_config in logits_processors_parser.items():\n        for keyword in logits_processor_config['arguments']:\n            if keyword in request_dict:\n                request_dict.pop(keyword)\n    return logits_processors\n</code></pre>"},{"location":"Serving/Serving/#src.happy_vllm.routers.functional.split_text","title":"<code>split_text(request)</code>  <code>async</code>","text":"<p>Splits a text</p> <p>The request should be a JSON object with the following fields: - text: The text to split - num_tokens_in_chunk (optional): The minimal number of tokens we want in each split - separators (optional) : The allowed separators between the chunks</p> Source code in <code>src/happy_vllm/routers/functional.py</code> <pre><code>@router.post(\"/split_text\")\nasync def split_text(request: Request):\n    \"\"\"Splits a text\n\n    The request should be a JSON object with the following fields:\n    - text: The text to split\n    - num_tokens_in_chunk (optional): The minimal number of tokens we want in each split\n    - separators (optional) : The allowed separators between the chunks\n    \"\"\"\n    model: Model = RESOURCES.get(RESOURCE_MODEL)\n\n    request_dict = await request.json()\n    split_text = model.split_text(**request_dict)\n    response = {\"split_text\": split_text}\n\n    return JSONResponse(response)\n</code></pre>"},{"location":"Serving/Serving/#src.happy_vllm.routers.functional.tokenizer","title":"<code>tokenizer(request)</code>  <code>async</code>","text":"<p>Tokenizes a text</p> <p>The request should be a JSON object with the following fields: - text: The text to tokenize - with_tokens_str (optional): Whether we want the tokens strings in the output - vanilla (optional) : Whether we want the vanilla version of the tokenizers</p> Source code in <code>src/happy_vllm/routers/functional.py</code> <pre><code>@router.post(\"/tokenizer\")\nasync def tokenizer(request: Request) -&gt; Response:\n    \"\"\"Tokenizes a text\n\n    The request should be a JSON object with the following fields:\n    - text: The text to tokenize\n    - with_tokens_str (optional): Whether we want the tokens strings in the output\n    - vanilla (optional) : Whether we want the vanilla version of the tokenizers\n    \"\"\"\n    model: Model = RESOURCES.get(RESOURCE_MODEL)\n    request_dict = await request.json()\n    text = request_dict.pop(\"text\")\n    vanilla = request_dict.get(\"vanilla\", True)\n    with_tokens_str = request_dict.get('with_tokens_str', False)\n\n    if vanilla:\n        tokens_ids = model._tokenizer(text)['input_ids']\n        if with_tokens_str:\n            tokens_str = model._tokenizer.convert_ids_to_tokens(tokens_ids)\n    else:\n        tokens_ids = model.tokenize(text)\n        if with_tokens_str:\n            tokens_str = [utils.proper_decode(model._tokenizer, token_id) for token_id in tokens_ids]\n\n\n    ret = {\"tokens_ids\": tokens_ids, \"tokens_nb\": len(tokens_ids)}\n    if with_tokens_str:\n        ret['tokens_str'] = tokens_str\n    return JSONResponse(ret)\n</code></pre>"}]}